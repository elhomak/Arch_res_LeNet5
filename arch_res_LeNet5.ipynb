{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b3b3b91",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b18f1289",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "\n",
    "# Define LeNet-5 architecture\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=33):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0)  # C1: 32x32 -> 28x28\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)  # S2: 28x28 -> 14x14\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)  # C3: 14x14 -> 10x10\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)  # S4: 10x10 -> 5x5\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5, stride=1, padding=0)  # C5: 5x5 -> 1x1\n",
    "        self.fc1 = nn.Linear(120, 84)  # F6\n",
    "        self.fc2 = nn.Linear(84, num_classes)  # Output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, 120)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Function to visualize feature maps\n",
    "def visualize_feature_maps(model, input_image, layer_name):\n",
    "    model.eval()\n",
    "    x = input_image.unsqueeze(0)\n",
    "    if layer_name == 'conv1':\n",
    "        x = F.relu(model.conv1(x))\n",
    "    elif layer_name == 'pool1':\n",
    "        x = F.relu(model.conv1(x))\n",
    "        x = model.pool1(x)\n",
    "    elif layer_name == 'conv2':\n",
    "        x = F.relu(model.conv1(x))\n",
    "        x = model.pool1(x)\n",
    "        x = F.relu(model.conv2(x))\n",
    "    elif layer_name == 'pool2':\n",
    "        x = F.relu(model.conv1(x))\n",
    "        x = model.pool1(x)\n",
    "        x = F.relu(model.conv2(x))\n",
    "        x = model.pool2(x)\n",
    "    feature_maps = x.detach().numpy()[0]\n",
    "    num_features = feature_maps.shape[0]\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(min(num_features, 6)):  # Visualize up to 6 feature maps\n",
    "        plt.subplot(1, 6, i+1)\n",
    "        plt.imshow(feature_maps[i], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.savefig(f'feature_maps_{layer_name}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device='cpu'):\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = correct / total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    return train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "# Plot loss and accuracy\n",
    "def plot_metrics(train_losses, val_losses, train_accs, val_accs):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Accuracy')\n",
    "    plt.plot(val_accs, label='Validation Accuracy')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig('training_metrics.png')\n",
    "    plt.close()\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(model, test_loader, device='cpu'):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "# Function to load Tifinagh dataset\n",
    "def load_tifinagh_dataset(data_dir, batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    # Define paths for train, validation, and test sets\n",
    "    train_dir = os.path.join(data_dir, 'train')\n",
    "    val_dir = os.path.join(data_dir, 'val')\n",
    "    test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "    # Check if directories exist\n",
    "    for directory in [train_dir, val_dir, test_dir]:\n",
    "        if not os.path.exists(directory):\n",
    "            raise FileNotFoundError(f\"Directory {directory} does not exist. Please check the dataset path.\")\n",
    "\n",
    "    # Load datasets\n",
    "    train_dataset = ImageFolder(train_dir, transform=transform)\n",
    "    val_dataset = ImageFolder(val_dir, transform=transform)\n",
    "    test_dataset = ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "    # Verify number of classes\n",
    "    if len(train_dataset.classes) != 33:\n",
    "        raise ValueError(f\"Expected 33 classes, but found {len(train_dataset.classes)} classes in the dataset.\")\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load Tifinagh dataset (replace with your dataset path)\n",
    "    data_dir = 'desktop/newfile'  # Update this to your dataset directory\n",
    "    try:\n",
    "        train_loader, val_loader, test_loader = load_tifinagh_dataset(data_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize model, loss, and optimizers\n",
    "    model = LeNet5(num_classes=33).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # SGD Optimizer\n",
    "    optimizer_sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "    # Adam Optimizer\n",
    "    optimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train with SGD\n",
    "    print(\"Training LeNet-5 with SGD...\")\n",
    "    train_losses_sgd, val_losses_sgd, train_accs_sgd, val_accs_sgd = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer_sgd, num_epochs=10, device=device\n",
    "    )\n",
    "\n",
    "    # Train with Adam (reset model weights)\n",
    "    model = LeNet5(num_classes=33).to(device)\n",
    "    print(\"Training LeNet-5 with Adam...\")\n",
    "    train_losses_adam, val_losses_adam, train_accs_adam, val_accs_adam = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer_adam, num_epochs=10, device=device\n",
    "    )\n",
    "\n",
    "    # Visualize metrics\n",
    "    plot_metrics(train_losses_adam, val_losses_adam, train_accs_adam, val_accs_adam)\n",
    "\n",
    "    # Visualize feature maps (using first image from test set)\n",
    "    first_image, _ = next(iter(test_loader))\n",
    "    visualize_feature_maps(model, first_image[0], 'conv1')\n",
    "    visualize_feature_maps(model, first_image[0], 'pool1')\n",
    "    visualize_feature_maps(model, first_image[0], 'conv2')\n",
    "    visualize_feature_maps(model, first_image[0], 'pool2')\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(model, test_loader, device)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb539b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.11/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ced7b938",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (649277900.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    apt install python3-torch\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "apt install python3-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19d2c492",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa600708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.11/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46b3d0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.11/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python3-seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bd78378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f726a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading dataset: [Errno 2] No such file or directory: 'Desktop\\x07mhcd-data-64/train'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Utility functions\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"Convert image to column matrix for convolution.\"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2 * pad - filter_h) // stride + 1\n",
    "    out_w = (W + 2 * pad - filter_w) // stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride * out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride * out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n",
    "    return col\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"Convert column matrix back to image.\"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2 * pad - filter_h) // stride + 1\n",
    "    out_w = (W + 2 * pad - filter_w) // stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "    img = np.zeros((N, C, H + 2 * pad, W + 2 * pad))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride * out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride * out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]\n",
    "\n",
    "# Layer implementations\n",
    "def conv_layer(x, W, b, stride=1, pad=0):\n",
    "    \"\"\"Convolution layer: x (N, C, H, W), W (F, C, FH, FW), b (F).\"\"\"\n",
    "    N, C, H, W = x.shape\n",
    "    F, _, FH, FW = W.shape\n",
    "    out_h = (H + 2 * pad - FH) // stride + 1\n",
    "    out_w = (W + 2 * pad - FW) // stride + 1\n",
    "\n",
    "    col = im2col(x, FH, FW, stride, pad)\n",
    "    col_W = W.reshape(F, -1).T\n",
    "    out = np.dot(col, col_W) + b\n",
    "    out = out.reshape(N, out_h, out_w, F).transpose(0, 3, 1, 2)\n",
    "    return out, (x, col)\n",
    "\n",
    "def conv_layer_backward(dout, cache, W, stride=1, pad=0):\n",
    "    \"\"\"Backward pass for convolution.\"\"\"\n",
    "    x, col = cache\n",
    "    F, C, FH, FW = W.shape\n",
    "    N, _, out_h, out_w = dout.shape\n",
    "\n",
    "    db = np.sum(dout, axis=(0, 2, 3))\n",
    "    dout = dout.transpose(0, 2, 3, 1).reshape(-1, F)\n",
    "    col_W = W.reshape(F, -1).T\n",
    "    dcol = np.dot(dout, col_W.T)\n",
    "    dW = np.dot(col.T, dout).reshape(W.shape)\n",
    "    dx = col2im(dcol, x.shape, FH, FW, stride, pad)\n",
    "    return dx, dW, db\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation.\"\"\"\n",
    "    return np.maximum(0, x), x\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "    \"\"\"Backward pass for ReLU.\"\"\"\n",
    "    return dout * (cache > 0)\n",
    "\n",
    "def avg_pool(x, kernel_size, stride):\n",
    "    \"\"\"Average pooling layer.\"\"\"\n",
    "    N, C, H, W = x.shape\n",
    "    out_h = (H - kernel_size) // stride + 1\n",
    "    out_w = (W - kernel_size) // stride + 1\n",
    "\n",
    "    col = im2col(x, kernel_size, kernel_size, stride, 0)\n",
    "    col = col.reshape(-1, kernel_size * kernel_size)\n",
    "    out = np.mean(col, axis=1)\n",
    "    out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "    return out, (x, kernel_size, stride)\n",
    "\n",
    "def avg_pool_backward(dout, cache):\n",
    "    \"\"\"Backward pass for average pooling.\"\"\"\n",
    "    x, kernel_size, stride = cache\n",
    "    N, C, H, W = x.shape\n",
    "    out_h = (H - kernel_size) // stride + 1\n",
    "    out_w = (W - kernel_size) // stride + 1\n",
    "\n",
    "    dout = dout.transpose(0, 2, 3, 1).reshape(-1, C)\n",
    "    dcol = np.repeat(dout / (kernel_size * kernel_size), kernel_size * kernel_size, axis=1)\n",
    "    dx = col2im(dcol, x.shape, kernel_size, kernel_size, stride, 0)\n",
    "    return dx\n",
    "\n",
    "def fc_layer(x, W, b):\n",
    "    \"\"\"Fully connected layer.\"\"\"\n",
    "    out = np.dot(x, W) + b\n",
    "    return out, (x,)\n",
    "\n",
    "def fc_layer_backward(dout, cache, W):\n",
    "    \"\"\"Backward pass for fully connected layer.\"\"\"\n",
    "    x, = cache\n",
    "    db = np.sum(dout, axis=0)\n",
    "    dW = np.dot(x.T, dout)\n",
    "    dx = np.dot(dout, W.T)\n",
    "    return dx, dW, db\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax activation.\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(pred, y):\n",
    "    \"\"\"Cross-entropy loss.\"\"\"\n",
    "    N = pred.shape[0]\n",
    "    log_pred = -np.log(pred[np.arange(N), y] + 1e-10)\n",
    "    loss = np.sum(log_pred) / N\n",
    "    grad = pred.copy()\n",
    "    grad[np.arange(N), y] -= 1\n",
    "    grad /= N\n",
    "    return loss, grad\n",
    "\n",
    "# LeNet-5 model\n",
    "class LeNet5:\n",
    "    def __init__(self, num_classes=33):\n",
    "        # Initialize weights and biases\n",
    "        self.params = {}\n",
    "        # C1: 1 input, 6 filters, 5x5\n",
    "        self.params['W1'] = np.random.randn(6, 1, 5, 5) * 0.01\n",
    "        self.params['b1'] = np.zeros(6)\n",
    "        # C3: 6 inputs, 16 filters, 5x5\n",
    "        self.params['W2'] = np.random.randn(16, 6, 5, 5) * 0.01\n",
    "        self.params['b2'] = np.zeros(16)\n",
    "        # C5: 16 inputs, 120 filters, 5x5\n",
    "        self.params['W3'] = np.random.randn(120, 16, 5, 5) * 0.01\n",
    "        self.params['b3'] = np.zeros(120)\n",
    "        # F6: 120 -> 84\n",
    "        self.params['W4'] = np.random.randn(120, 84) * 0.01\n",
    "        self.params['b4'] = np.zeros(84)\n",
    "        # Output: 84 -> 33\n",
    "        self.params['W5'] = np.random.randn(84, num_classes) * 0.01\n",
    "        self.params['b5'] = np.zeros(num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        cache = {}\n",
    "        # C1: 32x32x1 -> 28x28x6\n",
    "        out, cache['conv1'] = conv_layer(x, self.params['W1'], self.params['b1'])\n",
    "        out, cache['relu1'] = relu(out)\n",
    "        # S2: 28x28x6 -> 14x14x6\n",
    "        out, cache['pool1'] = avg_pool(out, kernel_size=2, stride=2)\n",
    "        # C3: 14x14x6 -> 10x10x16\n",
    "        out, cache['conv2'] = conv_layer(out, self.params['W2'], self.params['b2'])\n",
    "        out, cache['relu2'] = relu(out)\n",
    "        # S4: 10x10x16 -> 5x5x16\n",
    "        out, cache['pool2'] = avg_pool(out, kernel_size=2, stride=2)\n",
    "        # C5: 5x5x16 -> 1x1x120\n",
    "        out, cache['conv3'] = conv_layer(out, self.params['W3'], self.params['b3'])\n",
    "        out, cache['relu3'] = relu(out)\n",
    "        # Flatten: 1x1x120 -> 120\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        # F6: 120 -> 84\n",
    "        out, cache['fc1'] = fc_layer(out, self.params['W4'], self.params['b4'])\n",
    "        out, cache['relu4'] = relu(out)\n",
    "        # Output: 84 -> 33\n",
    "        out, cache['fc2'] = fc_layer(out, self.params['W5'], self.params['b5'])\n",
    "        # Softmax\n",
    "        scores = softmax(out)\n",
    "        cache['scores'] = scores\n",
    "        return scores, cache\n",
    "\n",
    "    def backward(self, dout, cache):\n",
    "        \"\"\"Backward pass.\"\"\"\n",
    "        grads = {}\n",
    "        # Output layer\n",
    "        dx, grads['W5'], grads['b5'] = fc_layer_backward(dout, cache['fc2'], self.params['W5'])\n",
    "        dx = relu_backward(dx, cache['relu4'])\n",
    "        # F6\n",
    "        dx, grads['W4'], grads['b4'] = fc_layer_backward(dx, cache['fc1'], self.params['W4'])\n",
    "        dx = dx.reshape(-1, 120, 1, 1)\n",
    "        # C5\n",
    "        dx = relu_backward(dx, cache['relu3'])\n",
    "        dx, grads['W3'], grads['b3'] = conv_layer_backward(dx, cache['conv3'], self.params['W3'])\n",
    "        # S4\n",
    "        dx = avg_pool_backward(dx, cache['pool2'])\n",
    "        # C3\n",
    "        dx = relu_backward(dx, cache['relu2'])\n",
    "        dx, grads['W2'], grads['b2'] = conv_layer_backward(dx, cache['conv2'], self.params['W2'])\n",
    "        # S2\n",
    "        dx = avg_pool_backward(dx, cache['pool1'])\n",
    "        # C1\n",
    "        dx = relu_backward(dx, cache['relu1'])\n",
    "        dx, grads['W1'], grads['b1'] = conv_layer_backward(dx, cache['conv1'], self.params['W1'])\n",
    "        return grads\n",
    "\n",
    "# Optimizer implementations\n",
    "class SGD:\n",
    "    def __init__(self, params, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.velocity = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "\n",
    "    def step(self, grads, params):\n",
    "        for key in params:\n",
    "            self.velocity[key] = self.momentum * self.velocity[key] - self.lr * grads[key]\n",
    "            params[key] += self.velocity[key]\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "        self.v = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self, grads, params):\n",
    "        self.t += 1\n",
    "        for key in params:\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]\n",
    "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key] ** 2)\n",
    "            m_hat = self.m[key] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[key] / (1 - self.beta2 ** self.t)\n",
    "            params[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "# Dataset loading\n",
    "def load_tifinagh_dataset(data_dir, split='train'):\n",
    "    \"\"\"Load Tifinagh dataset from .npy files.\"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    class_dirs = sorted(os.listdir(os.path.join(data_dir, split)))\n",
    "    if len(class_dirs) != 33:\n",
    "        raise ValueError(f\"Expected 33 classes, found {len(class_dirs)} in {split} set.\")\n",
    "\n",
    "    for class_idx, class_name in enumerate(class_dirs):\n",
    "        class_path = os.path.join(data_dir, split, class_name)\n",
    "        for img_path in [f for f in os.listdir(class_path) if f.endswith('.npy')]:\n",
    "            img = np.load(os.path.join(class_path, img_path))\n",
    "            if img.shape != (32, 32):\n",
    "                raise ValueError(f\"Image {img_path} has incorrect shape {img.shape}, expected (32, 32)\")\n",
    "            img = (img / 255.0 - 0.5) / 0.5  # Normalize to [-1, 1]\n",
    "            data.append(img)\n",
    "            labels.append(class_idx)\n",
    "\n",
    "    data = np.array(data)[:, np.newaxis, :, :]  # Shape: (N, 1, 32, 32)\n",
    "    labels = np.array(labels)\n",
    "    return data, labels\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_data, train_labels, val_data, val_labels, optimizer, num_epochs=10, batch_size=64):\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    N = train_data.shape[0]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle training data\n",
    "        indices = np.random.permutation(N)\n",
    "        train_data = train_data[indices]\n",
    "        train_labels = train_labels[indices]\n",
    "\n",
    "        # Training\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for i in range(0, N, batch_size):\n",
    "            batch_data = train_data[i:i + batch_size]\n",
    "            batch_labels = train_labels[i:i + batch_size]\n",
    "            scores, cache = model.forward(batch_data)\n",
    "            loss, dout = cross_entropy_loss(scores, batch_labels)\n",
    "            grads = model.backward(dout, cache)\n",
    "            optimizer.step(grads, model.params)\n",
    "            running_loss += loss * batch_data.shape[0]\n",
    "            predictions = np.argmax(scores, axis=1)\n",
    "            correct += np.sum(predictions == batch_labels)\n",
    "            total += batch_data.shape[0]\n",
    "\n",
    "        train_loss = running_loss / N\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        # Validation\n",
    "        val_scores, _ = model.forward(val_data)\n",
    "        val_loss, _ = cross_entropy_loss(val_scores, val_labels)\n",
    "        val_predictions = np.argmax(val_scores, axis=1)\n",
    "        val_acc = np.sum(val_predictions == val_labels) / len(val_labels)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "# Visualization functions\n",
    "def save_metrics(train_losses, val_losses, train_accs, val_accs):\n",
    "    \"\"\"Save loss and accuracy curves as CSV.\"\"\"\n",
    "    with open('training_metrics.csv', 'w') as f:\n",
    "        f.write('Epoch,Train Loss,Val Loss,Train Acc,Val Acc\\n')\n",
    "        for i in range(len(train_losses)):\n",
    "            f.write(f'{i+1},{train_losses[i]},{val_losses[i]},{train_accs[i]},{val_accs[i]}\\n')\n",
    "\n",
    "def save_confusion_matrix(model, test_data, test_labels):\n",
    "    \"\"\"Save confusion matrix as NumPy array.\"\"\"\n",
    "    scores, _ = model.forward(test_data)\n",
    "    predictions = np.argmax(scores, axis=1)\n",
    "    cm = np.zeros((33, 33), dtype=int)\n",
    "    for t, p in zip(test_labels, predictions):\n",
    "        cm[t, p] += 1\n",
    "    np.save('confusion_matrix.npy', cm)\n",
    "\n",
    "def save_feature_maps(model, input_image, layer_name):\n",
    "    \"\"\"Save feature maps as NumPy arrays.\"\"\"\n",
    "    input_image = input_image[np.newaxis, np.newaxis, :, :]  # Shape: (1, 1, 32, 32)\n",
    "    out = input_image\n",
    "    if layer_name == 'conv1':\n",
    "        out, _ = conv_layer(out, model.params['W1'], model.params['b1'])\n",
    "        out, _ = relu(out)\n",
    "    elif layer_name == 'pool1':\n",
    "        out, _ = conv_layer(out, model.params['W1'], model.params['b1'])\n",
    "        out, _ = relu(out)\n",
    "        out, _ = avg_pool(out, kernel_size=2, stride=2)\n",
    "    elif layer_name == 'conv2':\n",
    "        out, _ = conv_layer(out, model.params['W1'], model.params['b1'])\n",
    "        out, _ = relu(out)\n",
    "        out, _ = avg_pool(out, kernel_size=2, stride=2)\n",
    "        out, _ = conv_layer(out, model.params['W2'], model.params['b2'])\n",
    "        out, _ = relu(out)\n",
    "    elif layer_name == 'pool2':\n",
    "        out, _ = conv_layer(out, model.params['W1'], model.params['b1'])\n",
    "        out, _ = relu(out)\n",
    "        out, _ = avg_pool(out, kernel_size=2, stride=2)\n",
    "        out, _ = conv_layer(out, model.params['W2'], model.params['b2'])\n",
    "        out, _ = relu(out)\n",
    "        out, _ = avg_pool(out, kernel_size=2, stride=2)\n",
    "\n",
    "    np.save(f'feature_maps_{layer_name}.npy', out[0])\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Load dataset\n",
    "    data_dir = 'Desktop\\amhcd-data-64'  # Replace with actual path\n",
    "    try:\n",
    "        train_data, train_labels = load_tifinagh_dataset(data_dir, 'train')\n",
    "        val_data, val_labels = load_tifinagh_dataset(data_dir, 'val')\n",
    "        test_data, test_labels = load_tifinagh_dataset(data_dir, 'test')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize model\n",
    "    model = LeNet5(num_classes=33)\n",
    "\n",
    "    # Train with Adam\n",
    "    print(\"Training with Adam...\")\n",
    "    adam_optimizer = Adam(model.params, lr=0.001)\n",
    "    train_losses_adam, val_losses_adam, train_accs_adam, val_accs_adam = train_model(\n",
    "        model, train_data, train_labels, val_data, val_labels, adam_optimizer\n",
    "    )\n",
    "\n",
    "    # Save metrics\n",
    "    save_metrics(train_losses_adam, val_losses_adam, train_accs_adam, val_accs_adam)\n",
    "\n",
    "    # Save feature maps (using first test image)\n",
    "    first_image = test_data[0]\n",
    "    save_feature_maps(model, first_image, 'conv1')\n",
    "    save_feature_maps(model, first_image, 'pool1')\n",
    "    save_feature_maps(model, first_image, 'conv2')\n",
    "    save_feature_maps(model, first_image, 'pool2')\n",
    "\n",
    "    # Save confusion matrix\n",
    "    save_confusion_matrix(model, test_data, test_labels)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbcd5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
